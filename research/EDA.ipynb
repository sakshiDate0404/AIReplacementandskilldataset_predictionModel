{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59f079da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import Data Visualization Libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Import Filter Warning Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Import Logging\n",
    "import logging\n",
    "logging.basicConfig(level = logging.INFO,\n",
    "                    format = '%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filemode = 'w',\n",
    "                    filename = 'model.log',force = True)\n",
    "# Import Scikit Learn Libraries for Machine Learning Model Building\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,learning_curve,KFold\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Multicolinearity test and treatment libraries\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5341aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def data_ingestion():\n",
    "\n",
    "    logging.info(\"Data Ingestion Started...\")\n",
    "    df = pd.read_csv(r\"C:\\AIReplacementandskilldataset_predictionModel\\data\\raw\\AIREPLACEMENT.csv\")\n",
    "    logging.info(\"Data Ingestion Completed Successfully\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dc2d6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_exploration(df):\n",
    "    numerical_cols = df.select_dtypes(exclude='object').columns\n",
    "    stats=[]\n",
    "    for col in numerical_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        LW = Q1 - 1.5 * IQR\n",
    "        UW = Q3 + 1.5 * IQR\n",
    "\n",
    "        outlier_flag = \"Has Outliers\" if df[(df[col] < LW) | (df[col] > UW)].shape[0] > 0 else \"No Outliers\"\n",
    "\n",
    "        numerical_stats = OrderedDict({\n",
    "            \"Feature\": col,\n",
    "            \"Minimum\": df[col].min(),\n",
    "            \"Maximum\": df[col].max(),\n",
    "            \"Mean\": df[col].mean(),\n",
    "            \"Median\": df[col].median(),\n",
    "            \"Mode\": df[col].mode().iloc[0] if not df[col].mode().empty else np.nan,\n",
    "            \"25%\": Q1,\n",
    "            \"75%\": Q3,\n",
    "            \"IQR\": IQR,\n",
    "            \"Standard Deviation\": df[col].std(),\n",
    "            \"Skewness\": df[col].skew(),\n",
    "            \"Kurtosis\": df[col].kurt(),\n",
    "            \"Outlier Comment\": outlier_flag\n",
    "        })\n",
    "\n",
    "        stats.append(numerical_stats)\n",
    "\n",
    "    report = pd.DataFrame(stats)\n",
    "    return report\n",
    "\n",
    "def categorical_summary(df):\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "    summary = []\n",
    "    for col in cat_cols:\n",
    "        summary.append({\n",
    "            \"Feature\": col,\n",
    "            \"Unique Values\": df[col].nunique(),\n",
    "            \"Most Frequent\": df[col].mode().iloc[0] if not df[col].mode().empty else None,\n",
    "            \"Missing Values\": df[col].isna().sum()\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "427d6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, target_col, test_size=0.3, random_state=42):\n",
    "    X = data.drop(columns=[\"automation_risk_percent\"])\n",
    "    y = data[\"automation_risk_percent\"]\n",
    "\n",
    "    return train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "95ef2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(X_train, X_test):\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    cat_cols = X_train.select_dtypes(include=\"object\").columns\n",
    "\n",
    "    encoders = {}\n",
    "\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        # Fit ONLY on train\n",
    "        X_train[col] = le.fit_transform(X_train[col])\n",
    "\n",
    "        # Transform test using same mapping\n",
    "        X_test[col] = X_test[col].map(\n",
    "            lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "        )\n",
    "\n",
    "        encoders[col] = le\n",
    "\n",
    "    return X_train, X_test, encoders\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7b4852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def train_evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        return rmse, r2 # Corrected to return calculated r2 value\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error training/evaluating model {model.__class__.__name__}: {e}\")\n",
    "        return np.nan, np.nan # Return NaN for RMSE and R2 on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "923c0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Lasso\": Lasso(),\n",
    "        \"Ridge\": Ridge(),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(),\n",
    "        \"SVR\": SVR(),\n",
    "        \"KNN\": KNeighborsRegressor(),\n",
    "        \"Random Forest\": RandomForestRegressor(),\n",
    "        \"Gradient Boost\": GradientBoostingRegressor(),\n",
    "        \"Ada Boost\": AdaBoostRegressor(),\n",
    "        \"XG Boost\": XGBRegressor()\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        rmse, r2 = train_evaluate_model(\n",
    "            model, X_train, X_test, y_train, y_test\n",
    "        )\n",
    "        results.append([name, rmse, r2])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        results, columns=[\"Model Name\", \"RMSE\", \"R2 Score\"]).sort_values(\"R2 Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e1579546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(X_train, y_train, folds=10):\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Lasso\": Lasso(),\n",
    "        \"Ridge\": Ridge(),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(),\n",
    "        \"SVR\": SVR(),\n",
    "        \"KNN\": KNeighborsRegressor(),\n",
    "        \"Random Forest\": RandomForestRegressor(),\n",
    "        \"Gradient Boost\": GradientBoostingRegressor(),\n",
    "        \"Ada Boost\": AdaBoostRegressor(),\n",
    "        \"XG Boost\": XGBRegressor()\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(\n",
    "            model, X_train, y_train, cv=folds, scoring=\"r2\"\n",
    "        )\n",
    "        results.append([name, scores.mean(), scores.std()])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        results, columns=[\"Model Name\", \"CV Mean R2\", \"CV STD\"]).sort_values(\"CV Mean R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "361a5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(X_train, y_train, folds=5):\n",
    "    tuning_config = {\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBRegressor(),\n",
    "            \"params\": {\n",
    "                \"eta\": [0.1, 0.2, 0.3],\n",
    "                \"max_depth\": [3, 5, 7],\n",
    "                \"gamma\": [0, 10, 20],\n",
    "                \"reg_lambda\": [0, 1]\n",
    "            }\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            \"model\": RandomForestRegressor(),\n",
    "            \"params\": {\n",
    "                \"max_depth\": [5, 10, 15],\n",
    "                \"max_features\": [\"sqrt\", \"log2\", 3, 4]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "\n",
    "    for name, cfg in tuning_config.items():\n",
    "        grid = GridSearchCV(\n",
    "            cfg[\"model\"],\n",
    "            cfg[\"params\"],\n",
    "            cv=folds,\n",
    "            scoring=\"r2\",\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        best_models[name] = grid.best_estimator_\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2b35f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_tuning_cv(best_models, X_train, y_train, folds=5):\n",
    "    results = []\n",
    "\n",
    "    for name, model in best_models.items():\n",
    "        scores = cross_val_score(\n",
    "            model, X_train, y_train, cv=folds, scoring=\"r2\"\n",
    "        )\n",
    "        results.append([name, scores.mean(), scores.std()])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        results, columns=[\"Model Name\", \"CV Mean R2\", \"CV STD\"]).sort_values(\"CV Mean R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7cd4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test_evaluation(best_model, X_train, X_test, y_train, y_test):\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "83f40796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Stats Report:\n",
      "                        Feature   Minimum    Maximum          Mean     Median  \\\n",
      "0                        job_id      0.00   14999.00   7499.500000   7499.500   \n",
      "1                          year   2020.00    2026.00   2022.997200   2023.000   \n",
      "2       automation_risk_percent      5.00      94.98     46.176347     46.235   \n",
      "3          ai_replacement_score      4.01     113.07     46.155907     45.675   \n",
      "4               skill_gap_index      0.00      99.98     50.003708     49.930   \n",
      "5             salary_before_usd  30003.69  149984.06  89771.375196  89533.050   \n",
      "6              salary_after_usd  19022.67  191961.21  89870.633937  88787.330   \n",
      "7         salary_change_percent    -38.37      36.92      0.114268      0.150   \n",
      "8   skill_demand_growth_percent    -31.88      49.79      5.020461      4.960   \n",
      "9      remote_feasibility_score     10.01      99.99     54.898078     54.775   \n",
      "10            ai_adoption_level      0.01      99.98     49.798269     49.435   \n",
      "11  education_requirement_level      1.00       5.00      3.015400      3.000   \n",
      "\n",
      "        Mode         25%          75%         IQR  Standard Deviation  \\\n",
      "0       0.00   3749.7500   11249.2500   7499.5000         4330.271354   \n",
      "1    2025.00   2021.0000    2025.0000      4.0000            1.999365   \n",
      "2       5.00     28.7900      63.6025     34.8125           21.663635   \n",
      "3      33.69     28.3575      62.7125     34.3550           22.351347   \n",
      "4       4.05     25.1700      75.0250     49.8550           28.811040   \n",
      "5   41713.20  60127.2250  119823.8400  59696.6150        34522.125434   \n",
      "6   39290.69  59223.4575  118432.9675  59209.5100        35897.592992   \n",
      "7      -0.74     -6.6400       6.6900     13.3300           10.029197   \n",
      "8       4.88     -1.6625      11.7300     13.3925            9.956155   \n",
      "9      69.67     32.5175      77.4050     44.8875           25.863593   \n",
      "10     40.92     24.7075      74.7950     50.0875           28.860725   \n",
      "11      4.00      2.0000       4.0000      2.0000            1.405120   \n",
      "\n",
      "    Skewness  Kurtosis Outlier Comment  \n",
      "0   0.000000 -1.200000     No Outliers  \n",
      "1   0.002800 -1.252686     No Outliers  \n",
      "2   0.028868 -0.915808     No Outliers  \n",
      "3   0.169098 -0.712911     No Outliers  \n",
      "4   0.004201 -1.195994     No Outliers  \n",
      "5   0.013313 -1.198568     No Outliers  \n",
      "6   0.160875 -0.959567     No Outliers  \n",
      "7   0.023214 -0.004661    Has Outliers  \n",
      "8   0.013732  0.029004    Has Outliers  \n",
      "9   0.009792 -1.199904     No Outliers  \n",
      "10  0.006591 -1.202699     No Outliers  \n",
      "11 -0.013777 -1.286656     No Outliers  \n",
      "\n",
      "Categorical Stats Report:\n",
      "    Feature  Unique Values         Most Frequent  Missing Values\n",
      "0  job_role             10  Marketing Specialist               0\n",
      "1  industry              8               Finance               0\n",
      "2   country              9             Singapore               0\n",
      "\n",
      "Model Results:\n",
      "          Model Name       RMSE  R2 Score\n",
      "7     Gradient Boost   5.336732  0.939275\n",
      "6      Random Forest   5.419159  0.937385\n",
      "9           XG Boost   5.651739  0.931895\n",
      "1              Lasso   5.731254  0.929965\n",
      "0  Linear Regression   5.731762  0.929953\n",
      "2              Ridge   5.731762  0.929953\n",
      "8          Ada Boost   5.756233  0.929353\n",
      "3      Decision Tree   7.538822  0.878822\n",
      "4                SVR  21.656543  0.000015\n",
      "5                KNN  23.944046 -0.222392\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "          Model Name  CV Mean R2    CV STD\n",
      "7     Gradient Boost    0.941183  0.002599\n",
      "6      Random Forest    0.938679  0.003133\n",
      "9           XG Boost    0.932511  0.002605\n",
      "8          Ada Boost    0.930528  0.002544\n",
      "1              Lasso    0.929948  0.003487\n",
      "2              Ridge    0.929895  0.003499\n",
      "0  Linear Regression    0.929895  0.003499\n",
      "3      Decision Tree    0.880547  0.008316\n",
      "4                SVR   -0.002514  0.003315\n",
      "5                KNN   -0.206838  0.027038\n",
      "\n",
      "Best Models from Hyperparameter Tuning:\n",
      "{'XGBoost': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eta=0.1, eval_metric=None,\n",
      "             feature_types=None, feature_weights=None, gamma=20,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=None, n_jobs=None, ...), 'Random Forest': RandomForestRegressor(max_depth=15, max_features=4)}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logging.info(\"Starting data ingestion...\")\n",
    "    df = data_ingestion()\n",
    "    logging.info(\"Data ingestion completed.\")\n",
    "    logging.info(\"Starting data exploration...\")\n",
    "    numerical_stats_report = data_exploration(df)\n",
    "    categorical_stats_report = categorical_summary(df)\n",
    "    logging.info(\"Data exploration completed.\")\n",
    "    print(\"Numerical Stats Report:\")\n",
    "    print(numerical_stats_report)\n",
    "    print(\"\\nCategorical Stats Report:\")\n",
    "    print(categorical_stats_report)\n",
    "    logging.info(\"Starting data preprocessing...\")\n",
    "    # Replaced data_preprocessing with existing functions\n",
    "    X_train, X_test, y_train, y_test = split_data(\n",
    "        data=df,\n",
    "        target_col=\"automation_risk_percent\",\n",
    "        test_size=0.3,\n",
    "        random_state=42\n",
    "    )\n",
    "    X_train, X_test, encoders = encode_categorical(X_train, X_test)\n",
    "    logging.info(\"Data preprocessing completed.\")\n",
    "    logging.info(\"Starting model training...\")\n",
    "    # Replaced model_training with compare_models\n",
    "    results = compare_models(\n",
    "        X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    print(\"\\nModel Results:\")\n",
    "    print(results)\n",
    "    logging.info(\"Starting k-fold cross-validation...\")\n",
    "    cv_results = k_fold_cv(X_train, y_train)\n",
    "    logging.info(\"K-fold cross-validation completed.\")\n",
    "    print(\"\\nK-Fold Cross-Validation Results:\")\n",
    "    print(cv_results)\n",
    "    logging.info(\"Starting hyperparameter tuning...\")\n",
    "    best_models = hyperparameter_tuning(X_train, y_train,folds=5)\n",
    "    logging.info(\"Hyperparameter tuning completed.\")\n",
    "    print(\"\\nBest Models from Hyperparameter Tuning:\")\n",
    "    print(best_models)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aireplacementandskilldataset-predictionmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
